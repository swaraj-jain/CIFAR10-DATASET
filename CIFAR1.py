# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VdUcbRf0oZHGukP7dS-suqRqfQavtU5N
"""

import numpy as np
import pandas as pd
from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout
from keras.utils import np_utils
from keras.applications import VGG16
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

X_train.shape

y_train.shape

X_test.shape

plt.imshow(X_train[0])

X_train=X_train.astype('float32')
X_test=X_test.astype('float32')
X_train/=255
X_test/=255

plt.imshow(X_train[0])

"""one-hot encoding is done to coonert the given data to categorical data"""

Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)

print(y_train[0])

print(Y_train[0])

"""Initaliging the sequential Model"""

base_model = VGG16(weights='imagenet', include_top=False)
base_model.summary()

x = base_model.output

x.shape

x = GlobalAveragePooling2D()(x)

x.shape

x = Dense(128, activation='relu')(x)

x.shape

predictions = Dense(10, activation='softmax')(x)

for layer in base_model.layers:
    layer.trainable = False

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

model.fit(X_train, Y_train, epochs=20, batch_size=32)

for i, layer in enumerate(base_model.layers):
   print(i, layer.name)

for layer in model.layers:
   layer.trainable = True

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, Y_train, epochs=30, batch_size=42,validation_data=(X_test, Y_test))

pred = model.evaluate(X_test, Y_test)
accuracy_test=pred[1]

"""It is clear that the model is getting overfit,So we have to manage it,I can try it out many ideas like **:**

1.   Using Dropout
2.   List Try Different Model
3.   In pace of Random intilation of the Dense layer we can intilize by different way
4.   Using Different CostFunction
5.   Hyperparameter tunning by different ways(Grid,Random)
"""

